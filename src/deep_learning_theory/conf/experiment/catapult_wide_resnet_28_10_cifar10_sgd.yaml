# @package _global_

defaults:
  - override /datamodule: cifar10.yaml
  - override /backbone: wide_resnet_28_10.yaml
  - override /weight_init_fn: lecun_normal.yaml
  - override /loss: catapult_mse.yaml
  - override /optimizer: sgd.yaml
  - override /lr_scheduler: constant.yaml

datamodule:
  batch_size: 128

backbone:
  num_classes: 10

module:
  lr_scheduler_interval: step
  torch_compile: true

optimizer:
  lr: 0.25
  momentum: 0.0
  weight_decay: 0.0
  nesterov: false

lr_scheduler:
  total_iters: ${trainer.max_steps}

trainer:
  # Needs to be set to 1. Otherwise, we are not going to see the catapult.
  log_every_n_steps: 1
  max_steps: 50_000
  logger:
    - _target_: lightning.pytorch.loggers.CSVLogger
      save_dir: lightning_logs
      name: super_convergence
    - _target_: lightning.pytorch.loggers.WandbLogger
      project: deep-learning-theory
      tags: ['catapult']
